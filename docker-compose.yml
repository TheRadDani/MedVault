version: '3.9'

# Updated: Feb 2026

services:
  # Ollama LLM infra - with GPU support
  ollama:
    image: ollama/ollama:latest  # Using latest stable version (2.6.0 not available on Docker Hub)
    container_name: medvault_ollama
    restart: unless-stopped
    
    # GPU support - using runtime instead of gpus option for compatibility
    runtime: nvidia  # Requires: nvidia-docker2 installed on host
    
    # Ports
    ports:
      - "11434:11434"  # Ollama API endpoint
    
    # Persistent model storage
    volumes:
      - ollama_data:/root/.ollama
      - ./models:/models  # Optional: mount custom models
    
    # Health check - ensures app waits for Ollama readiness
    healthcheck:
      test: ["CMD", "bash", "-c", "echo > /dev/tcp/localhost/11434"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    
    # Environment - production-grade settings
    environment:
      OLLAMA_HOST: 0.0.0.0:11434  # Listen on all interfaces inside container
      OLLAMA_NUM_PARALLEL: 4  # Parallel request handling
      OLLAMA_NUM_GPU: -1  # Use all available VRAM
      OLLAMA_KEEP_ALIVE: 5m  # Keep model in memory 5 minutes after last request
      CUDA_VISIBLE_DEVICES: 0  # Adjust if using specific GPUs
    
    # Resource limits for production stability
    cpus: '4.0'
    mem_limit: 16g
    memswap_limit: 20g  # Allow some swap for stability
    
    # Logging configuration with rotation
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
        labels: "service=ollama"
    
    networks:
      - mednet
  
  # RAG Application (Streamlit)
  app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        PYTHON_VERSION: "3.11"  # Specify Python version
    image: medvault_app:1.0.0  # Tag for reproducible deployments
    container_name: medvault_app
    restart: unless-stopped
    
    # Ports
    ports:
      - "8501:8501"  # Streamlit default port
    
    # Wait for Ollama to be healthy before starting
    depends_on:
      ollama:
        condition: service_healthy
    
    # Environment configuration
    environment:
      OLLAMA_BASE_URL: http://ollama:11434  # Use service name (DNS)
      STREAMLIT_SERVER_PORT: 8501
      STREAMLIT_SERVER_HEADLESS: 'true'
      STREAMLIT_SERVER_RUNONSAVE: 'false'  # Disable auto-reload in prod
      STREAMLIT_LOGGER_LEVEL: info
      PYTHONUNBUFFERED: '1'  # Real-time logging
      HF_HOME: /tmp/huggingface  # Cache HuggingFace models in /tmp (tmpfs)
    
    # Resource limits
    cpus: '2.0'
    mem_limit: 8g
    
    # Volumes - use read-only where possible
    volumes:
      - ./:/app  # Mount entire project for access to src/, app/, data/
      - ./data:/app/data  # Writable data directory
      - ./vector_store:/app/vector_store  # Writable vector store
      - ./logs:/app/logs  # Writable logs directory
    
    # Health check
    healthcheck:
      test: ["CMD", "bash", "-c", "echo > /dev/tcp/localhost/8501"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
        labels: "service=app"
    
    # Tmpfs for Streamlit cache (doesn't need to persist)
    tmpfs:
      - /tmp
      - /run
      - /home/appuser/.streamlit
      - /home/appuser/.cache
    
    networks:
      - mednet

# Named volume for persistent Ollama model storage
volumes:
  ollama_data:
    driver: local

# Network for service communication
networks:
  mednet:
    driver: bridge